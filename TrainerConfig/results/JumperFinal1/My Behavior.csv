Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,High Score,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,0.6883384,-0.21791172,218.6727272727273,-0.9177272683517499,-0.9177272683517499,6.0,0.019333271,0.021731006,0.00028458497,0.19486164,0.0047435965,1.0
100000,0.68415844,-0.24388105,220.70403587443946,-0.9143497713010408,-0.9143497713010408,10.0,0.01876117,0.024361182,0.00025679378,0.18559793,0.0042813364,1.0
150000,0.6520723,-0.25345448,243.38308457711443,-0.8960198960568181,-0.8960198960568181,None,0.017156973,0.023414236,0.00022595597,0.17531864,0.0037684005,1.0
200000,0.63370466,-0.27281326,228.61333333333334,-0.9088888842529721,-0.9088888842529721,None,0.017466187,0.023646194,0.00019513372,0.16504456,0.0032557235,1.0
250000,0.64592725,-0.27377266,256.1701030927835,-0.8902061812810063,-0.8902061812810063,None,0.015705924,0.025007498,0.00016431895,0.15477297,0.0027431704,1.0
300000,0.63930047,-0.27600133,225.69406392694063,-0.9041095831214565,-0.9041095831214565,None,0.016867328,0.023692973,0.00013350019,0.14450005,0.0022305518,1.0
350000,0.6276903,-0.28066406,233.38834951456312,-0.9024509754324076,-0.9024509754324076,11.0,0.015477161,0.02350977,0.00010266547,0.13422182,0.0017176677,1.0
400000,0.5936347,-0.26371965,241.83644859813083,-0.8972222176897857,-0.8972222176897857,None,0.016979875,0.021869762,7.4915035e-05,0.12497166,0.0012560852,1.0
450000,0.6060851,-0.27033752,224.05963302752295,-0.911467885308036,-0.911467885308036,None,0.017190363,0.025836831,4.7105244e-05,0.11570172,0.0007935158,1.0
500000,0.5805026,-0.2631337,227.26339285714286,-0.906726452200402,-0.906726452200402,None,0.017742768,0.023139184,1.6220976e-05,0.10540696,0.0002798073,1.0
